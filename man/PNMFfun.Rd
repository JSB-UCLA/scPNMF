% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/pNMF_func.R
\name{PNMFfun}
\alias{PNMFfun}
\title{Fitting PNMF Models}
\usage{
PNMFfun(
  X,
  rank = 10,
  tol = 0.001,
  maxIter = 500,
  verboseN = FALSE,
  zerotol = 1e-10,
  method = "KL",
  label = NULL,
  mu = 1,
  lambda = 0.01,
  seed = 123
)
}
\arguments{
\item{X}{Input data matrix, where rows represent features (genes), columns represent samples (cells).}

\item{rank}{Specification of the factorization rank (number of low dimension).}

\item{tol}{A threshold below which would be considered converged. Default is 1e-3.}

\item{maxIter}{Number of max iteration times. Default is 500.}

\item{verboseN}{A boolean value indicating whether to print number of iterations.}

\item{zerotol}{A threshold on basis loadings below which would be considered zero. Default is 1e-10.}

\item{method}{A character string indicating which method to be used. One of \code{"EucDist"}, \code{"KL"}, or \code{"DPNMF"}.}

\item{label}{A character vector indicating the cluster type for each cell. Required only when \code{method = "DPNMF"}.}

\item{mu}{A numerical value which controls the penalty term. Larger \code{mu} represents haivier penalization of class distances in \code{DPNMF}. Default is 1.}

\item{lambda}{A numerical value which controls the magnituide of within class distances. Larger \code{lambda} represents larger proportion of within class distances in the total penalty term. Default is 0.01.}

\item{seed}{Random seed of the initialization.}
}
\value{
A list with components:
\describe{
  \item{\code{basis}}{The basis of model fit (\eqn{W}).}
  \item{\code{ld}}{The mapped scores (\eqn{W^TX}), which is the dimension reduced result.}
}
}
\description{
Fast Projective Nonnegative Matrix Factorization Realizatiton based on Euclidean Distance / KL Divergence / Discriminant pNMF.
}
\details{
Given a data matrix (rows as features and columns as samples), this function
computes the Projective Nonnegative Matrix Factorization (PNMF). Based on different objective functions,
the choices are Euclidean distance (\code{"EucDist"}), KL divergence (\code{"KL"}) (Yang, Zhirong, and Erkki Oja. 2010), or Discriminant PNMF (\code{"DPNMF"}) (Guan, Naiyang, et al. 2013). 
\code{"EucDist"} is supposed to be the most common one;
\code{"KL"} is similar to KL-NMF (Poisson-NMF), and may work better for count data; 
\code{"DPNMF"} requires the predefined labels.

The fitting result of PNMF shares characteristics of both PCA and NMF. The model returns
a \code{basis} matrix, which is similar to the loading matrix in PCA. However, notice that
unlike in PCA the first PC always represents the largest variation, each basis vectors are equal in PNMF.
}
\references{
\itemize{
\item Yang, Z., & Oja, E. (2010). Linear and nonlinear projective nonnegative matrix factorization. IEEE Transactions on Neural Networks, 21(5), 734-749.
\item Guan, N., Zhang, X., Luo, Z., Tao, D., & Yang, X. (2013). Discriminant projective non-negative matrix factorization. PloS one, 8(12), e83291.
\item \url{https://github.com/richardbeare/pNMF}
}
}
\author{
Kexin Li, \email{aileenlikexin@outlook.com}

Dongyuan Song, \email{dongyuansong@g.ucla.edu}
}
